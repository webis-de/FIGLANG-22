
Loading distilbert tokenizer
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/Users/msen/Documents/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
1,000 training samples
  128 validation samples
  301 test samples
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/Users/msen/Documents/venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
======== Epoch 1 / 6 ========
Training...
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.924416542053223
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.907711029052734
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.875246047973633
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.895658493041992
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.861377716064453
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.8094916343688965
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.8240766525268555
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.7836809158325195
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.7860565185546875
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.825767993927002
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.738924503326416
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.725691318511963
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.693618297576904
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.738982677459717
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.78502893447876
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.7406487464904785
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.55106258392334
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.646451473236084
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.6470232009887695
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.652506351470947
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.570502281188965
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.479398727416992
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.430309772491455
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.414167404174805
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.493233680725098
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.380336284637451
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.5050811767578125
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.531343936920166
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.4604997634887695
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.513697147369385
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.424102783203125
INPUT SHAPE:  torch.Size([8, 128])
INPUT MASK SHAPE:  torch.Size([8, 128])
TARGET SHAPE:  torch.Size([8])
Current Loss 4.419720649719238
Average training loss: 4.66
Training epoch took: 0:01:21
Running Validation...
  Accuracy: 0.25
  Validation Loss: 4.28
  Validation took: 0:00:03
======== Epoch 2 / 6 ========
Training...
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.483496189117432
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.276696681976318
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.437022686004639
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.090480804443359
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.219275951385498
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
Current Loss 4.33392333984375
INPUT SHAPE:  torch.Size([32, 128])
INPUT MASK SHAPE:  torch.Size([32, 128])
TARGET SHAPE:  torch.Size([32])
